{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d938a9f",
   "metadata": {},
   "source": [
    "Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2338d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import copy\n",
    "import pickle as pickle \n",
    "import random as rn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4d14c",
   "metadata": {},
   "source": [
    "Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy tk pickle-mixin matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87834e1b",
   "metadata": {},
   "source": [
    "Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable initialization\n",
    "grid_size = 3\n",
    "N_episodes = 1000\n",
    "decay = 1             #decay = 1 for epsilon decay, decay = 0 for greedy epsilon\n",
    "epsilon = 0.5         # epsilon in the case where decay = 0\n",
    "sym = 1               # sym = 1 to activate symmetry incorporation, sym = 0 otherwise\n",
    "epsilon_init = 1      #initialize initial epsilon (1 for complete random moves)\n",
    "epsilon_end = 0      #initialize initial epsilon (0 for complete greedy moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39440b0d",
   "metadata": {},
   "source": [
    "Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, master, player1, player2,N_episodes=0,decay =1,epsilon=0,epsilon_init=1,epsilon_end=0,sym =0, Q_learn=None, Q={}, alpha=0.3, gamma=0.9):\n",
    "        frame = tk.Frame()\n",
    "        frame.grid()\n",
    "        self.master = master\n",
    "        master.title(\"Tic Tac Toe\")\n",
    "        self.player1 = player1\n",
    "        self.player2 = player2\n",
    "        self.current_player = player1\n",
    "        self.other_player = player2\n",
    "        self.empty_text = \"\"\n",
    "        self.board = Board()\n",
    "        self.nsteps=0\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_init = epsilon_init\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.decay = decay\n",
    "        self.win_X_count = 0\n",
    "        self.win_X_100 = []\n",
    "        self.win_O_count = 0\n",
    "        self.win_O_100 = []\n",
    "        self.draw_count = 0\n",
    "        self.draw_100 = []\n",
    "        self.total = 0\n",
    "        self.total_100 = []\n",
    "        self.change_list = []    \n",
    "        if grid_size == 3:\n",
    "            self.mask = np.array([[(0,0),((0,1)),(0,2)],[(1,0),(1,1),(1,2)],[(2,0),(2,1),(2,2)]])\n",
    "        elif grid_size == 4:\n",
    "            self.mask = np.array([[(0,0),((0,1)),(0,2),(0,3)],[(1,0),(1,1),(1,2),(1,3)],[(2,0),(2,1),(2,2),(2,3)],\n",
    "                              [(3,0),(3,1),(3,2),(3,3)]])\n",
    "        self.mask_rot90cc = np.rot90(self.mask,1)\n",
    "        self.mask_rot180cc = np.rot90(self.mask,2)\n",
    "        self.mask_rot270cc = np.rot90(self.mask,3)\n",
    "        self.mask_flipx = np.flip(self.mask,0)\n",
    "        self.mask_flipy = np.flip(self.mask,1)\n",
    "        self.learn = sym\n",
    "\n",
    "        # creating the buttons for the tic tac toe games\n",
    "        self.buttons = [[None for _ in range(grid_size)] for _ in range(grid_size)]\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                self.buttons[i][j] = tk.Button(frame, height=grid_size, width=grid_size, text=self.empty_text, command=lambda i=i, j=j: self.callback(self.buttons[i][j]))\n",
    "                self.buttons[i][j].grid(row=i, column=j)\n",
    "        #creating reset button\n",
    "        self.reset_button = tk.Button(text=\"Reset\", command=self.reset)\n",
    "        self.reset_button.grid(row=grid_size)\n",
    "\n",
    "        self.Q_learn = Q_learn\n",
    "        if self.Q_learn:\n",
    "            self.Q = Q\n",
    "            self.alpha = alpha          # Learning rate\n",
    "            self.gamma = gamma          # Discount rate\n",
    "            self.share_Q_with_players()\n",
    "        \n",
    "    @property\n",
    "    def Q_learn(self):\n",
    "        if self._Q_learn is not None:    \n",
    "            return self._Q_learn\n",
    "        if isinstance(self.player1, QPlayer) or isinstance(self.player2, QPlayer):\n",
    "            return True\n",
    "\n",
    "    @Q_learn.setter\n",
    "    def Q_learn(self, _Q_learn):\n",
    "        self._Q_learn = _Q_learn\n",
    "        \n",
    "    def share_Q_with_players(self):             # The action value table Q is shared with the QPlayers to help them make their move decisions\n",
    "        if isinstance(self.player1, QPlayer):\n",
    "            self.player1.Q = self.Q\n",
    "        if isinstance(self.player2, QPlayer):\n",
    "            self.player2.Q = self.Q\n",
    "    \n",
    "    #\n",
    "    def callback(self, button):\n",
    "        if self.board.over():\n",
    "            pass                # Do nothing if the game is already over\n",
    "        else:       \n",
    "            # if the current player is the human player and the other player is the computer player\n",
    "            if isinstance(self.current_player, HumanPlayer) and isinstance(self.other_player, ComputerPlayer):\n",
    "                computer_player = self.other_player\n",
    "                if self.empty(button):\n",
    "                    human_move = self.get_move(button)\n",
    "                    self.handle_move(human_move)\n",
    "                    if not self.board.over():               # Trigger the computer's next move\n",
    "                        computer_move = computer_player.get_move(self.board)\n",
    "                        self.handle_move(computer_move)\n",
    "\n",
    "    def empty(self, button):\n",
    "        return button[\"text\"] == self.empty_text\n",
    "\n",
    "    def get_move(self, button):\n",
    "        info = button.grid_info()\n",
    "        # Get move coordinates from the button's metadata\n",
    "        move = (int(info[\"row\"]), int(info[\"column\"]))\n",
    "        return move\n",
    "\n",
    "    def handle_move(self, move):\n",
    "        if self.Q_learn: \n",
    "            self.learn_Q(move)\n",
    "        i, j = move         # Get row and column number of the corresponding button\n",
    "        self.buttons[i][j].configure(text=self.current_player.mark)     # Change the label on the button to the current player's mark\n",
    "        self.board.place_mark(move, self.current_player.mark)           # Update the board\n",
    "        if self.board.over():\n",
    "            self.declare_outcome()\n",
    "        else:\n",
    "            self.switch_players()\n",
    "\n",
    "    def declare_outcome(self):\n",
    "        self.total = self.total+1\n",
    "        if self.board.winner() is None:\n",
    "            self.draw_count = self.draw_count+1\n",
    "            print(\"Cat's game.\")\n",
    "        else:\n",
    "            if self.current_player.mark == \"X\":\n",
    "                self.win_X_count = self.win_X_count+1\n",
    "            elif self.current_player.mark == \"O\":\n",
    "                self.win_O_count = self.win_O_count+1    \n",
    "            print((\"The game is over. The player with mark {mark} won!\".format(mark=self.current_player.mark)))\n",
    "            \n",
    "        if (self.nsteps+1)%100 == 0 and self.nsteps>0:\n",
    "            self.total_100 = self.total_100 + [self.total]\n",
    "            self.draw_100 = self.draw_100 + [self.draw_count]\n",
    "            self.win_X_100 = self.win_X_100 + [self.win_X_count]\n",
    "            self.win_O_100 = self.win_O_100 + [self.win_O_count]\n",
    "\n",
    "    def reset(self):\n",
    "#         print(\"Resetting...\")\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                self.buttons[i][j].configure(text=self.empty_text)\n",
    "        self.board = Board(grid=np.ones((grid_size,grid_size))*np.nan)\n",
    "        self.current_player = self.player1\n",
    "        self.other_player = self.player2\n",
    "#         np.random.seed(seed=0)      # Set the random seed to zero to see the Q-learning 'in action' or for debugging purposes\n",
    "        self.play(nsteps=self.nsteps)\n",
    "\n",
    "    def switch_players(self):\n",
    "        if self.current_player == self.player1:\n",
    "            self.current_player = self.player2\n",
    "            self.other_player = self.player1\n",
    "        else:\n",
    "            self.current_player = self.player1\n",
    "            self.other_player = self.player2\n",
    "\n",
    "    def play(self,nsteps=0):\n",
    "        if self.decay ==1:\n",
    "            self.nsteps=nsteps\n",
    "            r = max((N_episodes-self.nsteps)/N_episodes,0)\n",
    "            epsilon = (self.epsilon_init-self.epsilon_end)*r+self.epsilon_end\n",
    "        else: epsilon = self.epsilon\n",
    "        self.total = 0    \n",
    "        self.change_total = 0\n",
    "        if isinstance(self.player1, HumanPlayer) and isinstance(self.player2, ComputerPlayer):\n",
    "            pass\n",
    "        # Make the two computer players play against each other without button presses\n",
    "        elif isinstance(self.player1, ComputerPlayer) and isinstance(self.player2, ComputerPlayer):\n",
    "            if isinstance (self.player1, QPlayer) and isinstance(self.player2, QPlayer):\n",
    "                self.player1.epsilon = epsilon\n",
    "                self.player2.epsilon = epsilon\n",
    "                while not self.board.over():\n",
    "                    self.play_turn()\n",
    "            elif isinstance (self.player1, QPlayer) and isinstance(self.player2, RandomPlayer):\n",
    "                self.player1.epsilon = epsilon\n",
    "                self.player1.randomMoveNo = 0\n",
    "                self.player1.greedyMoveNo = 0\n",
    "                while not self.board.over():\n",
    "                    self.play_turn()    \n",
    "        \n",
    "        if self.total!=0:\n",
    "#             print(self.change_total,self.total)\n",
    "            self.change_list = self.change_list + [self.change_total]\n",
    "            \n",
    "\n",
    "\n",
    "    def play_turn(self):\n",
    "        move = self.current_player.get_move(self.board) \n",
    "        self.handle_move(move)\n",
    "        \n",
    "    # If Q-learning is toggled on, \"learn_Q\" should be called after receiving a move from an instance of Player \n",
    "    # and before implementing the move (using Board's \"place_mark\" method)\n",
    "    def learn_Q(self, move): \n",
    "        state_key = QPlayer.make_and_maybe_add_key(self.board, self.current_player.mark, self.Q)\n",
    "        next_board = self.board.get_next_board(move, self.current_player.mark)\n",
    "        reward = next_board.give_reward()\n",
    "        next_state_key = QPlayer.make_and_maybe_add_key(next_board, self.other_player.mark, self.Q)\n",
    "        \n",
    "            \n",
    "        if next_board.over():\n",
    "            expected = reward\n",
    "        else:\n",
    "            next_Qs = self.Q[next_state_key]             # The Q values represent the expected future reward for player X for each available move in the next state (after the move has been made)\n",
    "            if self.current_player.mark == \"X\":\n",
    "                expected = reward + (self.gamma * min(next_Qs.values()))        # If the current player is X, the next player is O, and the move with the minimum Q value should be chosen according to our \"sign convention\"\n",
    "            elif self.current_player.mark == \"O\":\n",
    "                expected = reward + (self.gamma * max(next_Qs.values()))        # If the current player is O, the next player is X, and the move with the maximum Q vlue should be chosen\n",
    "        \n",
    "        \n",
    "        change = self.alpha * (expected - self.Q[state_key][move])\n",
    "        self.Q[state_key][move] += change\n",
    "        self.change_total = self.change_total + change\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.learn == 1:\n",
    "            r = rn.random()\n",
    "            state_key_dict = {}\n",
    "            default_Qvalue = 0.5 +  r\n",
    "            if default_Qvalue>1:\n",
    "                default_Qvalue = 1\n",
    "            #90 degree rotation counterclockwise\n",
    "            state_key_90cc = self.board.make_key(np.rot90(self.board.grid),self.current_player.mark)\n",
    "            if self.Q.get(state_key_90cc) is None:\n",
    "                moves = self.board.available_moves_sym(np.rot90(self.board.grid))\n",
    "                # assigns values to the Q value of the state\n",
    "                self.Q[state_key_90cc] = {move: default_Qvalue for move in moves} \n",
    "            \n",
    "            if self.Q.get(state_key_90cc) is not None and state_key!=state_key_90cc:\n",
    "                for i in range(grid_size):\n",
    "                    for j in range(grid_size):\n",
    "                        if self.mask_rot90cc[i,j][0]==move[0] and self.mask_rot90cc[i,j][1]==move[1]:\n",
    "                            move90cc = (i,j)         \n",
    "                if state_key_90cc not in state_key_dict:\n",
    "                    state_key_dict[state_key_90cc]=move90cc\n",
    "#                     print(\"90\",state_key,move,state_key_90cc,move90cc)\n",
    "            \n",
    "                \n",
    "                \n",
    "            #180 degree rotation counterclockwise\n",
    "            state_key_180cc = self.board.make_key(np.rot90(self.board.grid,2),self.current_player.mark)\n",
    "            if self.Q.get(state_key_180cc) is None:\n",
    "                moves = self.board.available_moves_sym(np.rot90(self.board.grid,2))\n",
    "                # assigns values to the Q value of the state\n",
    "                self.Q[state_key_180cc] = {move: default_Qvalue for move in moves} \n",
    "            if self.Q.get(state_key_180cc) is not None and state_key!=state_key_180cc:\n",
    "                for i in range(grid_size):\n",
    "                    for j in range(grid_size):\n",
    "                        if self.mask_rot180cc[i,j][0]==move[0] and self.mask_rot180cc[i,j][1]==move[1]:\n",
    "                            move180cc = (i,j)\n",
    "                if state_key_180cc not in state_key_dict:\n",
    "                    state_key_dict[state_key_180cc]=move180cc\n",
    "#                     print(\"180\",state_key,move,state_key_180cc,move180cc)\n",
    "                \n",
    "                \n",
    "            #270 degree rotation counterclockwise\n",
    "            state_key_270cc = self.board.make_key(np.rot90(self.board.grid,3),self.current_player.mark)\n",
    "            if self.Q.get(state_key_270cc) is None:\n",
    "                moves = self.board.available_moves_sym(np.rot90(self.board.grid,3))\n",
    "                # assigns values to the Q value of the state\n",
    "                self.Q[state_key_270cc] = {move: default_Qvalue for move in moves} \n",
    "            if self.Q.get(state_key_270cc) is not None and state_key!=state_key_270cc:\n",
    "                for i in range(grid_size):\n",
    "                    for j in range(grid_size):\n",
    "                        if self.mask_rot270cc[i,j][0]==move[0] and self.mask_rot270cc[i,j][1]==move[1]:\n",
    "                            move270cc = (i,j)\n",
    "                if state_key_270cc not in state_key_dict:\n",
    "                    state_key_dict[state_key_270cc]=move270cc\n",
    "#                     print(\"270\",state_key,move,state_key_270cc,move270cc)\n",
    "                \n",
    "            #flipx degree rotation counterclockwise\n",
    "            state_key_flipx = self.board.make_key(np.flip(self.board.grid,0),self.current_player.mark)\n",
    "            if self.Q.get(state_key_flipx) is None:\n",
    "                moves = self.board.available_moves_sym(np.flip(self.board.grid,0))\n",
    "                # assigns values to the Q value of the state\n",
    "                self.Q[state_key_flipx] = {move: default_Qvalue for move in moves} \n",
    "            \n",
    "            if self.Q.get(state_key_flipx) is not None and state_key!=state_key_flipx:\n",
    "                for i in range(grid_size):\n",
    "                    for j in range(grid_size):\n",
    "                        if self.mask_flipx[i,j][0]==move[0] and self.mask_flipx[i,j][1]==move[1]:\n",
    "                            moveflipx = (i,j)\n",
    "                if state_key_flipx not in state_key_dict:\n",
    "                    state_key_dict[state_key_flipx]=moveflipx\n",
    "#                     print(\"flipx\",state_key,move,state_key_flipx,moveflipx)\n",
    "                \n",
    "                \n",
    "            #flipy degree rotation counterclockwise\n",
    "            state_key_flipy = self.board.make_key(np.flip(self.board.grid,1),self.current_player.mark)\n",
    "            if self.Q.get(state_key_flipy) is None:\n",
    "                moves = self.board.available_moves_sym(np.flip(self.board.grid,1))\n",
    "                # assigns values to the Q value of the state\n",
    "                self.Q[state_key_flipy] = {move: default_Qvalue for move in moves} \n",
    "            if self.Q.get(state_key_flipy) is not None and state_key!=state_key_flipy:\n",
    "                for i in range(grid_size):\n",
    "                    for j in range(grid_size):\n",
    "                        if self.mask_flipy[i,j][0]==move[0] and self.mask_flipy[i,j][1]==move[1]:\n",
    "                            moveflipy = (i,j)\n",
    "                if state_key_flipy not in state_key_dict:\n",
    "                    state_key_dict[state_key_flipy]=moveflipy\n",
    "#                     print(\"flipy\",state_key,move,state_key_flipy,moveflipy)\n",
    "                \n",
    "            for key in state_key_dict:\n",
    "                change = self.alpha * (expected - self.Q[key][state_key_dict[key]])\n",
    "                self.Q[key][state_key_dict[key]] += change\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class Board:\n",
    "    def __init__(self, grid=np.ones((grid_size,grid_size))*np.nan):\n",
    "        self.grid = grid\n",
    "\n",
    "    def winner(self):\n",
    "        rows = [self.grid[i,:] for i in range(grid_size)]\n",
    "        cols = [self.grid[:,j] for j in range(grid_size)]\n",
    "        diag = [np.array([self.grid[i,i] for i in range(grid_size)])]\n",
    "        cross_diag = [np.array([self.grid[2-i,i] for i in range(grid_size)])]\n",
    "        lanes = np.concatenate((rows, cols, diag, cross_diag))      # A \"lane\" is defined as a row, column, diagonal, or cross-diagonal\n",
    "\n",
    "        any_lane = lambda x: any([np.array_equal(lane, x) for lane in lanes])   # Returns true if any lane is equal to the input argument \"x\"\n",
    "        if any_lane(np.ones(grid_size)):\n",
    "            return \"X\"\n",
    "        elif any_lane(np.zeros(grid_size)):\n",
    "            return \"O\"\n",
    "        \n",
    "    # The game is over if there is a winner or if no squares remain empty (cat's game)\n",
    "    def over(self):             \n",
    "        return (not np.any(np.isnan(self.grid))) or (self.winner() is not None)\n",
    "\n",
    "    def place_mark(self, move, mark):       # Place a mark on the board\n",
    "        num = Board.mark2num(mark)\n",
    "        self.grid[tuple(move)] = num\n",
    "\n",
    "    @staticmethod\n",
    "    def mark2num(mark):         # Convert's a player's mark to a number to be inserted in the Numpy array representing the board. The mark must be either \"X\" or \"O\".\n",
    "        d = {\"X\": 1, \"O\": 0}\n",
    "        return d[mark]\n",
    "\n",
    "    def available_moves(self):\n",
    "        # returns the coordinates with nan values in the grid\n",
    "        return [(i,j) for i in range(grid_size) for j in range(grid_size) if np.isnan(self.grid[i][j])]\n",
    "\n",
    "    def available_moves_sym(self,grid):\n",
    "        # returns the coordinates with nan values in the grid\n",
    "        return [(i,j) for i in range(grid_size) for j in range(grid_size) if np.isnan(grid[i][j])]\n",
    "\n",
    "\n",
    "    def get_next_board(self, move, mark):\n",
    "        next_board = copy.deepcopy(self)\n",
    "        next_board.place_mark(move, mark)\n",
    "        return next_board\n",
    "    # For Q-learning, returns a 10-character string representing the state of the board and the player whose turn it is\n",
    "    def make_key(self,grid, mark):          \n",
    "        fill_value = grid_size**2\n",
    "        filled_grid = copy.deepcopy(grid)  #https://www.geeksforgeeks.org/copy-python-deep-copy-shallow-copy/\n",
    "        np.place(filled_grid, np.isnan(filled_grid), fill_value) #https://www.geeksforgeeks.org/numpy-place-python/\n",
    "        return \"\".join(map(str, (list(map(int, filled_grid.flatten()))))) + mark \n",
    "\n",
    "    def give_reward(self):                          # Assign a reward for the player with mark X in the current board position.\n",
    "        if self.over():\n",
    "            if self.winner() is not None:\n",
    "                if self.winner() == \"X\":\n",
    "                    return 1.0                      # Player X won -> positive reward\n",
    "                elif self.winner() == \"O\":\n",
    "                    return -1.0                     # Player O won -> negative reward\n",
    "            else:\n",
    "                return 0.5                          # A smaller positive reward for cat's game\n",
    "        else:\n",
    "            return 0.0                              # No reward if the game is not yet finished\n",
    "\n",
    "class Player(object):\n",
    "    def __init__(self, mark):\n",
    "        self.mark = mark\n",
    "\n",
    "    @property\n",
    "    def opponent_mark(self):\n",
    "        if self.mark == 'X':\n",
    "            return 'O'\n",
    "        elif self.mark == 'O':\n",
    "            return 'X'\n",
    "        else:\n",
    "            print(\"The player's mark must be either 'X' or 'O'.\")\n",
    "\n",
    "class HumanPlayer(Player):\n",
    "    pass\n",
    "class ComputerPlayer(Player):\n",
    "    pass\n",
    "class RandomPlayer(ComputerPlayer):\n",
    "    @staticmethod\n",
    "    def get_move(board):\n",
    "        moves = board.available_moves()\n",
    "        # If \"moves\" is not an empty list (as it would be if cat's game were reached)\n",
    "        if moves:   \n",
    "            # Apply random selection to the index, as otherwise it will be seen as a 2D array\n",
    "            return moves[np.random.choice(len(moves))]    \n",
    "        \n",
    "\n",
    "    \n",
    "class QPlayer(ComputerPlayer):\n",
    "    #initialization of a Qplayer\n",
    "    def __init__(self, mark, Q={}):\n",
    "        super(QPlayer, self).__init__(mark=mark)\n",
    "        self.Q = Q\n",
    "        self.randomMoveNo = 0\n",
    "        self.greedyMoveNo = 0\n",
    "    \n",
    "    def get_move(self, board):\n",
    "        # With probability epsilon, choose a move at random (\"epsilon-greedy\" exploration)\n",
    "        if np.random.uniform() < epsilon:\n",
    "            self.randomMoveNo = self.randomMoveNo + 1\n",
    "            moves = board.available_moves()\n",
    "            move = moves[np.random.choice(len(moves))] \n",
    "            return move\n",
    "        else:\n",
    "            self.greedyMoveNo = self.greedyMoveNo + 1\n",
    "            state_key = QPlayer.make_and_maybe_add_key(board,self.mark, self.Q) # gets the state of the game\n",
    "            Qs = self.Q[state_key] # gets the q values of the state_key obtained\n",
    "            if self.mark == \"X\":\n",
    "                return QPlayer.stochastic_argminmax(Qs, max)\n",
    "            elif self.mark == \"O\":\n",
    "                return QPlayer.stochastic_argminmax(Qs, min)\n",
    "\n",
    "    @staticmethod\n",
    "    # Make a dictionary key for the current state (board + player turn) and if Q does not yet have it, add it to Q\n",
    "    def make_and_maybe_add_key(board, mark, Q):  \n",
    "        r = rn.random()\n",
    "        # Encourages exploration\n",
    "        default_Qvalue = 0.5 +  r\n",
    "        if default_Qvalue>1:\n",
    "            default_Qvalue=1\n",
    "        state_key = board.make_key(board.grid,mark) # gets the state of the board (10 element str) and the player playing \n",
    "        \n",
    "        if Q.get(state_key) is None:\n",
    "                        moves = board.available_moves()\n",
    "                        # assigns values to the Q value of the state\n",
    "                        Q[state_key] = {move: default_Qvalue for move in moves}    # The available moves in each state are initially given a default value of zero\n",
    "                    \n",
    "        return state_key\n",
    "\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    # Determines either the argmin or argmax of the array Qs such that if there are 'ties', one is chosen at random\n",
    "    def stochastic_argminmax(Qs, min_or_max):       \n",
    "        min_or_maxQ = min_or_max(list(Qs.values()))\n",
    "        # If there is more than one move corresponding to the maximum Q-value, choose one at random\n",
    "        if list(Qs.values()).count(min_or_maxQ) > 1:      \n",
    "            best_options = [move for move in list(Qs.keys()) if Qs[move] == min_or_maxQ]\n",
    "            move = best_options[np.random.choice(len(best_options))]\n",
    "        else:\n",
    "            move = min_or_max(Qs, key=Qs.get)\n",
    "        return move\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddacab6f",
   "metadata": {},
   "source": [
    "Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "player1 = QPlayer(mark=\"X\")\n",
    "player2 = QPlayer(mark=\"O\")\n",
    "\n",
    "game = Game(root, player1, player2,N_episodes = N_episodes,decay = 1,sym=1,epsilon=epsilon)\n",
    "NoOfrandom = []\n",
    "NoOfgreedy =[]\n",
    "total = []\n",
    "for episodes in range(N_episodes):\n",
    "    print(\"Episode:\",episodes)\n",
    "    game.play(nsteps=episodes)\n",
    "    game.reset()\n",
    "Q = game.Q\n",
    "filename = \"Q_epsilon_09_Nepisodes_{} (3by3).p\".format(N_episodes)\n",
    "pickle.dump(Q, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fe1c8",
   "metadata": {},
   "source": [
    "Cell 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b012f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = pickle.load(open(\"Q_epsilon_09_Nepisodes_\"+str(N_episodes)+\" (3by3).p\", \"rb\"))\n",
    "\n",
    "root = tk.Tk()\n",
    "player1 = HumanPlayer(mark=\"X\")\n",
    "player2 = QPlayer(mark=\"O\")\n",
    "\n",
    "game = Game(root, player1, player2, Q=Q,decay=0)\n",
    "\n",
    "game.play()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74a7c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
